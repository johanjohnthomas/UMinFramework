{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umin_framework_title"
   },
   "source": [
    "# UMinFramework: Augmented LLM Benchmarking Demo\n",
    "\n",
    "This notebook demonstrates the complete UMinFramework workflow for comparing baseline Large Language Models (LLMs) against **AugmentedLLMs** that integrate:\n",
    "\n",
    "- üîç **Prompt Refinement**: Clarifies ambiguous prompts using fine-tuned models\n",
    "- üéØ **Uncertainty Quantification**: Real-time confidence monitoring during generation  \n",
    "- üîÑ **Continuous Chain-of-Thought Backtracking**: Automatic reasoning injection when uncertainty is high\n",
    "\n",
    "## What You'll Learn\n",
    "- How to set up the UMinFramework environment\n",
    "- Download and prepare coding benchmark datasets (HumanEval, MBPP)\n",
    "- Run comparative benchmarks between baseline and augmented LLMs\n",
    "- Analyze results using pass@k metrics and detailed performance statistics\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab with GPU runtime (recommended)\n",
    "- Basic understanding of Python and machine learning concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "# üöÄ Environment Setup\n",
    "\n",
    "First, let's set up our Colab environment with GPU acceleration and install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Consider switching to GPU runtime for better performance.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repository"
   },
   "outputs": [],
   "source": [
    "# Clone the UMinFramework repository\n",
    "!git clone https://github.com/johanjohnthomas/UMinFramework.git\n",
    "%cd UMinFramework\n",
    "\n",
    "# List the project structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "print(\"Installing Python dependencies...\")\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install additional dependencies that might be needed\n",
    "!pip install datasets evaluate accelerate\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully!\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import transformers\n",
    "    import datasets\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "    print(f\"Datasets version: {datasets.__version__}\")\n",
    "    print(\"\\nüéâ All core dependencies are available!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_setup_section"
   },
   "source": [
    "# üìä Data Setup and Download\n",
    "\n",
    "Let's download and prepare the benchmark datasets using the provided automation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_datasets"
   },
   "outputs": [],
   "source": [
    "# Download benchmark datasets\n",
    "print(\"Downloading HumanEval, MBPP, and AskCQ datasets...\")\n",
    "!python scripts/download_datasets.py\n",
    "\n",
    "print(\"\\nDataset download completed!\")\n",
    "print(\"\\nAvailable datasets:\")\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect_datasets"
   },
   "outputs": [],
   "source": [
    "# Inspect the downloaded datasets\n",
    "import json\n",
    "\n",
    "def inspect_dataset(filename, dataset_name):\n",
    "    \"\"\"Display sample data from a dataset file.\"\"\"\n",
    "    try:\n",
    "        with open(f\"data/{filename}\", 'r') as f:\n",
    "            # For JSONL files, read first line\n",
    "            if filename.endswith('.jsonl'):\n",
    "                sample = json.loads(f.readline().strip())\n",
    "            else:\n",
    "                sample = json.load(f)\n",
    "                if isinstance(sample, list) and len(sample) > 0:\n",
    "                    sample = sample[0]\n",
    "        \n",
    "        print(f\"\\nüìã {dataset_name} Sample:\")\n",
    "        print(\"=\"*50)\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"{key}: {value[:100]}...\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {filename}: {e}\")\n",
    "\n",
    "# Inspect each dataset\n",
    "inspect_dataset(\"humaneval.jsonl\", \"HumanEval\")\n",
    "inspect_dataset(\"mbpp.jsonl\", \"MBPP\")\n",
    "inspect_dataset(\"askcq.jsonl\", \"AskCQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "framework_demo_section"
   },
   "source": [
    "# üß™ UMinFramework Components Demo\n",
    "\n",
    "Let's explore the core components of the UMinFramework before running the full benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_framework"
   },
   "outputs": [],
   "source": [
    "# Import UMinFramework components\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "try:\n",
    "    from umin_framework import (\n",
    "        AugmentedLLM, \n",
    "        AugmentedLLMConfig,\n",
    "        UncertaintyHead,\n",
    "        GenerationLoop,\n",
    "        SafeCodeExecutor\n",
    "    )\n",
    "    print(\"‚úÖ UMinFramework components imported successfully!\")\n",
    "    \n",
    "    # Display available components\n",
    "    print(\"\\nüì¶ Available Components:\")\n",
    "    print(\"- AugmentedLLM: Main wrapper class\")\n",
    "    print(\"- AugmentedLLMConfig: Configuration management\")\n",
    "    print(\"- UncertaintyHead: Token-level uncertainty quantification\")\n",
    "    print(\"- GenerationLoop: Uncertainty-aware generation with backtracking\")\n",
    "    print(\"- SafeCodeExecutor: Secure code execution sandbox\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure the UMinFramework is properly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_code_executor"
   },
   "outputs": [],
   "source": [
    "# Demo: Safe Code Execution\n",
    "print(\"üîí Safe Code Executor Demo\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "executor = SafeCodeExecutor(timeout=5.0)\n",
    "\n",
    "# Test 1: Safe code\n",
    "safe_code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "result = fibonacci(7)\n",
    "print(f\"Fibonacci(7) = {result}\")\n",
    "\"\"\"\n",
    "\n",
    "test_code = \"assert fibonacci(7) == 13\"\n",
    "\n",
    "print(\"Testing safe code:\")\n",
    "result = executor.execute(safe_code, test_code)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Output: {result.output}\")\n",
    "print(f\"Execution time: {result.execution_time:.3f}s\")\n",
    "\n",
    "# Test 2: Dangerous code (should be blocked)\n",
    "print(\"\\nTesting dangerous code:\")\n",
    "dangerous_code = \"import os; os.system('rm -rf /')\"\n",
    "result = executor.execute(dangerous_code)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "print(\"\\n‚úÖ Dangerous code successfully blocked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_basic_augmented_llm"
   },
   "outputs": [],
   "source": [
    "# Demo: Basic AugmentedLLM Usage\n",
    "print(\"ü§ñ AugmentedLLM Basic Demo\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create configuration (without prompt refinement for this demo)\n",
    "config = AugmentedLLMConfig(\n",
    "    generation_model=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Small model for demo\n",
    "    enable_prompt_refinement=False,  # Disable for this demo\n",
    "    enable_uncertainty_monitoring=True,\n",
    "    enable_backtracking=True,\n",
    "    uncertainty_threshold=0.8,\n",
    "    max_length=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Model: {config.generation_model}\")\n",
    "print(f\"- Uncertainty threshold: {config.uncertainty_threshold}\")\n",
    "print(f\"- Backtracking enabled: {config.enable_backtracking}\")\n",
    "print(f\"- Max length: {config.max_length}\")\n",
    "\n",
    "try:\n",
    "    # Initialize AugmentedLLM\n",
    "    print(\"\\nInitializing AugmentedLLM...\")\n",
    "    augmented_llm = AugmentedLLM(config=config)\n",
    "    print(\"‚úÖ AugmentedLLM initialized successfully!\")\n",
    "    \n",
    "    # Test generation\n",
    "    test_prompt = \"def add_two_numbers(a, b):\"\n",
    "    print(f\"\\nTest prompt: {test_prompt}\")\n",
    "    print(\"Generating...\")\n",
    "    \n",
    "    result = augmented_llm.generate(test_prompt, return_metadata=True)\n",
    "    \n",
    "    print(f\"\\nüìù Generated code:\")\n",
    "    print(result['text'])\n",
    "    print(f\"\\nüìä Metadata:\")\n",
    "    print(f\"- Tokens generated: {result['generated_tokens']}\")\n",
    "    print(f\"- Average uncertainty: {result['avg_uncertainty']:.3f}\")\n",
    "    print(f\"- Backtrack events: {result['backtrack_events']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"This might be due to limited Colab resources. The benchmark script will work better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prompt_refiner_section"
   },
   "source": [
    "# üîß Prompt Refiner Training (Optional)\n",
    "\n",
    "The UMinFramework includes a prompt refinement component that clarifies ambiguous prompts. Let's see if we can train it quickly or use a pre-trained version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_prompt_refiner"
   },
   "outputs": [],
   "source": [
    "# Check if prompt refiner model exists\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "refiner_path = Path(\"models/prompt_refiner\")\n",
    "\n",
    "if refiner_path.exists() and (refiner_path / \"config.json\").exists():\n",
    "    print(\"‚úÖ Prompt refiner model found!\")\n",
    "    print(f\"Location: {refiner_path}\")\n",
    "    \n",
    "    # List model files\n",
    "    print(\"\\nModel files:\")\n",
    "    for file in refiner_path.iterdir():\n",
    "        if file.is_file():\n",
    "            print(f\"- {file.name}\")\n",
    "    \n",
    "    prompt_refiner_available = True\n",
    "else:\n",
    "    print(\"‚ùå Prompt refiner model not found.\")\n",
    "    print(\"\\nTo train the prompt refiner (takes 10-15 minutes):\")\n",
    "    print(\"Uncomment and run the training cell below.\")\n",
    "    prompt_refiner_available = False\n",
    "\n",
    "print(f\"\\nPrompt refiner available: {prompt_refiner_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_prompt_refiner"
   },
   "outputs": [],
   "source": [
    "# Uncomment to train the prompt refiner (optional - takes time)\n",
    "# ‚ö†Ô∏è This will take 10-15 minutes and requires significant GPU memory\n",
    "\n",
    "# print(\"üîß Training Prompt Refiner...\")\n",
    "# print(\"This may take 10-15 minutes with GPU acceleration.\")\n",
    "\n",
    "# !python scripts/finetune_prompt_refiner.py \\\n",
    "#     --model_name google/flan-t5-small \\\n",
    "#     --output_dir models/prompt_refiner \\\n",
    "#     --num_train_epochs 3 \\\n",
    "#     --per_device_train_batch_size 8 \\\n",
    "#     --save_strategy epoch \\\n",
    "#     --logging_steps 10\n",
    "\n",
    "# print(\"‚úÖ Prompt refiner training completed!\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Prompt refiner training skipped for this demo.\")\n",
    "print(\"The benchmark will run without prompt refinement.\")\n",
    "print(\"Uncomment the code above to train the prompt refiner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark_section"
   },
   "source": [
    "# üìà Running the Benchmark\n",
    "\n",
    "Now let's run the core benchmarking suite to compare baseline and augmented LLMs on coding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_baseline_only"
   },
   "outputs": [],
   "source": [
    "# Run baseline-only benchmark first (faster)\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è Running Baseline-Only Benchmark\")\n",
    "print(\"=\"*50)\n",
    "print(\"This will evaluate a baseline LLM on the coding datasets.\")\n",
    "print(\"Estimated time: 2-5 minutes\\n\")\n",
    "\n",
    "!python scripts/run_benchmark.py \\\n",
    "    --baseline-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "    --data-path data \\\n",
    "    --output-dir results/colab_baseline \\\n",
    "    --no-augmented \\\n",
    "    --max-length 150 \\\n",
    "    --temperature 0.2 \\\n",
    "    --timeout 20 \\\n",
    "    --verbose\n",
    "\n",
    "print(\"\\n‚úÖ Baseline benchmark completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_augmented"
   },
   "outputs": [],
   "source": [
    "# Run augmented benchmark (if prompt refiner is available)\n",
    "if prompt_refiner_available:\n",
    "    print(\"üöÄ Running Augmented LLM Benchmark\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"This will compare baseline vs AugmentedLLM with all features enabled.\")\n",
    "    print(\"Estimated time: 5-10 minutes\\n\")\n",
    "    \n",
    "    !python scripts/run_benchmark.py \\\n",
    "        --baseline-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "        --augmented-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "        --prompt-refiner-model models/prompt_refiner \\\n",
    "        --data-path data \\\n",
    "        --output-dir results/colab_augmented \\\n",
    "        --uncertainty-threshold 0.7 \\\n",
    "        --backtrack-window 3 \\\n",
    "        --max-length 150 \\\n",
    "        --temperature 0.2 \\\n",
    "        --timeout 20 \\\n",
    "        --verbose\n",
    "    \n",
    "    print(\"\\n‚úÖ Augmented benchmark completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Augmented benchmark skipped (no prompt refiner available)\")\n",
    "    print(\"Running augmented benchmark without prompt refinement...\")\n",
    "    \n",
    "    !python scripts/run_benchmark.py \\\n",
    "        --baseline-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "        --augmented-model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "        --data-path data \\\n",
    "        --output-dir results/colab_augmented \\\n",
    "        --uncertainty-threshold 0.7 \\\n",
    "        --backtrack-window 3 \\\n",
    "        --max-length 150 \\\n",
    "        --temperature 0.2 \\\n",
    "        --timeout 20 \\\n",
    "        --verbose\n",
    "    \n",
    "    print(\"\\n‚úÖ Augmented benchmark (without prompt refinement) completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "# üìä Results Analysis\n",
    "\n",
    "Let's analyze the benchmark results and visualize the performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_results"
   },
   "outputs": [],
   "source": [
    "# Load and display benchmark results\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_benchmark_results(results_dir):\n",
    "    \"\"\"Load benchmark results from JSON file.\"\"\"\n",
    "    results_file = Path(results_dir) / \"benchmark_results.json\"\n",
    "    \n",
    "    if not results_file.exists():\n",
    "        print(f\"‚ùå Results file not found: {results_file}\")\n",
    "        return None\n",
    "    \n",
    "    with open(results_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def display_results_summary(data, title):\n",
    "    \"\"\"Display a summary of benchmark results.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\"*len(title))\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data available\")\n",
    "        return\n",
    "    \n",
    "    # Display metadata\n",
    "    meta = data.get('metadata', {})\n",
    "    print(f\"üìä Evaluation Summary:\")\n",
    "    print(f\"   Model: {meta.get('baseline_model', 'Unknown')}\")\n",
    "    print(f\"   Total problems: {meta.get('total_results', 0)}\")\n",
    "    print(f\"   Max length: {meta.get('max_length', 'N/A')}\")\n",
    "    print(f\"   Temperature: {meta.get('temperature', 'N/A')}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    stats = data.get('statistics', {})\n",
    "    \n",
    "    if 'baseline' in stats and stats['baseline']:\n",
    "        baseline = stats['baseline']\n",
    "        print(f\"\\nü§ñ Baseline Results:\")\n",
    "        print(f\"   Pass rate: {baseline.get('pass_rate', 0):.1%}\")\n",
    "        print(f\"   Avg generation time: {baseline.get('avg_generation_time', 0):.2f}s\")\n",
    "        print(f\"   Avg tokens: {baseline.get('avg_tokens', 0):.1f}\")\n",
    "    \n",
    "    if 'augmented' in stats and stats['augmented']:\n",
    "        augmented = stats['augmented']\n",
    "        print(f\"\\nüöÄ Augmented Results:\")\n",
    "        print(f\"   Pass rate: {augmented.get('pass_rate', 0):.1%}\")\n",
    "        print(f\"   Avg generation time: {augmented.get('avg_generation_time', 0):.2f}s\")\n",
    "        print(f\"   Avg tokens: {augmented.get('avg_tokens', 0):.1f}\")\n",
    "        print(f\"   Avg uncertainty: {augmented.get('avg_uncertainty', 0):.3f}\")\n",
    "        print(f\"   Avg backtracks: {augmented.get('avg_backtrack_events', 0):.1f}\")\n",
    "    \n",
    "    # Display pass@k metrics\n",
    "    if 'pass_at_k' in data:\n",
    "        pass_k = data['pass_at_k']\n",
    "        \n",
    "        if 'baseline' in pass_k and pass_k['baseline']['overall']:\n",
    "            print(f\"\\nüìà Baseline Pass@k:\")\n",
    "            for k, score in pass_k['baseline']['overall'].items():\n",
    "                print(f\"   Pass@{k}: {score:.3f}\")\n",
    "        \n",
    "        if 'augmented' in pass_k and pass_k['augmented']['overall']:\n",
    "            print(f\"\\nüéØ Augmented Pass@k:\")\n",
    "            for k, score in pass_k['augmented']['overall'].items():\n",
    "                print(f\"   Pass@{k}: {score:.3f}\")\n",
    "\n",
    "# Load baseline results\n",
    "baseline_data = load_benchmark_results(\"results/colab_baseline\")\n",
    "display_results_summary(baseline_data, \"üìä BASELINE RESULTS\")\n",
    "\n",
    "# Load augmented results (if available)\n",
    "augmented_data = load_benchmark_results(\"results/colab_augmented\")\n",
    "if augmented_data:\n",
    "    display_results_summary(augmented_data, \"üöÄ AUGMENTED RESULTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_analysis"
   },
   "outputs": [],
   "source": [
    "# Detailed analysis with visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_comparison_chart(baseline_data, augmented_data):\n",
    "    \"\"\"Create comparison charts for baseline vs augmented results.\"\"\"\n",
    "    \n",
    "    if not baseline_data or not augmented_data:\n",
    "        print(\"‚ùå Cannot create comparison chart - missing data\")\n",
    "        return\n",
    "    \n",
    "    # Extract statistics\n",
    "    baseline_stats = baseline_data.get('statistics', {}).get('baseline', {})\n",
    "    augmented_stats = augmented_data.get('statistics', {}).get('augmented', {})\n",
    "    \n",
    "    if not baseline_stats or not augmented_stats:\n",
    "        print(\"‚ùå Cannot create comparison chart - missing statistics\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Baseline vs Augmented LLM Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Pass Rate Comparison\n",
    "    models = ['Baseline', 'Augmented']\n",
    "    pass_rates = [\n",
    "        baseline_stats.get('pass_rate', 0) * 100,\n",
    "        augmented_stats.get('pass_rate', 0) * 100\n",
    "    ]\n",
    "    \n",
    "    bars1 = ax1.bar(models, pass_rates, color=['#ff7f7f', '#7fbf7f'])\n",
    "    ax1.set_ylabel('Pass Rate (%)')\n",
    "    ax1.set_title('Success Rate Comparison')\n",
    "    ax1.set_ylim(0, max(pass_rates) * 1.2 if max(pass_rates) > 0 else 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rate in zip(bars1, pass_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Generation Time Comparison\n",
    "    gen_times = [\n",
    "        baseline_stats.get('avg_generation_time', 0),\n",
    "        augmented_stats.get('avg_generation_time', 0)\n",
    "    ]\n",
    "    \n",
    "    bars2 = ax2.bar(models, gen_times, color=['#ff7f7f', '#7fbf7f'])\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.set_title('Average Generation Time')\n",
    "    \n",
    "    for bar, time in zip(bars2, gen_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{time:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Token Count Comparison\n",
    "    token_counts = [\n",
    "        baseline_stats.get('avg_tokens', 0),\n",
    "        augmented_stats.get('avg_tokens', 0)\n",
    "    ]\n",
    "    \n",
    "    bars3 = ax3.bar(models, token_counts, color=['#ff7f7f', '#7fbf7f'])\n",
    "    ax3.set_ylabel('Token Count')\n",
    "    ax3.set_title('Average Tokens Generated')\n",
    "    \n",
    "    for bar, tokens in zip(bars3, token_counts):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{tokens:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Pass@k Comparison (if available)\n",
    "    baseline_pass_k = baseline_data.get('pass_at_k', {}).get('baseline', {}).get('overall', {})\n",
    "    augmented_pass_k = augmented_data.get('pass_at_k', {}).get('augmented', {}).get('overall', {})\n",
    "    \n",
    "    if baseline_pass_k and augmented_pass_k:\n",
    "        k_values = list(baseline_pass_k.keys())\n",
    "        baseline_scores = [baseline_pass_k[k] for k in k_values]\n",
    "        augmented_scores = [augmented_pass_k.get(k, 0) for k in k_values]\n",
    "        \n",
    "        x = np.arange(len(k_values))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax4.bar(x - width/2, baseline_scores, width, label='Baseline', color='#ff7f7f')\n",
    "        ax4.bar(x + width/2, augmented_scores, width, label='Augmented', color='#7fbf7f')\n",
    "        \n",
    "        ax4.set_xlabel('k')\n",
    "        ax4.set_ylabel('Pass@k Score')\n",
    "        ax4.set_title('Pass@k Metrics Comparison')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels([f'k={k}' for k in k_values])\n",
    "        ax4.legend()\n",
    "        ax4.set_ylim(0, 1.1)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Pass@k data\\nnot available', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Pass@k Metrics Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print improvement summary\n",
    "    print(\"\\nüéØ PERFORMANCE IMPROVEMENT SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    baseline_pass = baseline_stats.get('pass_rate', 0)\n",
    "    augmented_pass = augmented_stats.get('pass_rate', 0)\n",
    "    \n",
    "    if baseline_pass > 0:\n",
    "        improvement = (augmented_pass - baseline_pass) / baseline_pass * 100\n",
    "        print(f\"Pass Rate Improvement: {improvement:+.1f}%\")\n",
    "    else:\n",
    "        print(\"Pass Rate Improvement: Cannot calculate (baseline = 0)\")\n",
    "    \n",
    "    baseline_time = baseline_stats.get('avg_generation_time', 0)\n",
    "    augmented_time = augmented_stats.get('avg_generation_time', 0)\n",
    "    \n",
    "    if baseline_time > 0:\n",
    "        time_change = (augmented_time - baseline_time) / baseline_time * 100\n",
    "        print(f\"Generation Time Change: {time_change:+.1f}%\")\n",
    "    \n",
    "    print(f\"Average Uncertainty Score: {augmented_stats.get('avg_uncertainty', 0):.3f}\")\n",
    "    print(f\"Average Backtrack Events: {augmented_stats.get('avg_backtrack_events', 0):.1f}\")\n",
    "\n",
    "# Create comparison visualization\n",
    "if baseline_data and augmented_data:\n",
    "    create_comparison_chart(baseline_data, augmented_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create comparison chart - missing benchmark data\")\n",
    "    print(\"Make sure both baseline and augmented benchmarks completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_results_table"
   },
   "outputs": [],
   "source": [
    "# Create detailed results table\n",
    "def create_results_table(baseline_data, augmented_data):\n",
    "    \"\"\"Create a detailed comparison table.\"\"\"\n",
    "    \n",
    "    # Load CSV files if available\n",
    "    baseline_csv = Path(\"results/colab_baseline/benchmark_results.csv\")\n",
    "    augmented_csv = Path(\"results/colab_augmented/benchmark_results.csv\")\n",
    "    \n",
    "    try:\n",
    "        if baseline_csv.exists():\n",
    "            baseline_df = pd.read_csv(baseline_csv)\n",
    "            print(\"üìã BASELINE DETAILED RESULTS\")\n",
    "            print(\"=\"*40)\n",
    "            print(baseline_df[['problem_id', 'dataset', 'baseline_passed', \n",
    "                             'baseline_generation_time', 'baseline_tokens_generated']].to_string(index=False))\n",
    "        \n",
    "        if augmented_csv.exists():\n",
    "            augmented_df = pd.read_csv(augmented_csv)\n",
    "            print(\"\\nüöÄ AUGMENTED DETAILED RESULTS\")\n",
    "            print(\"=\"*40)\n",
    "            print(augmented_df[['problem_id', 'dataset', 'augmented_passed', \n",
    "                              'augmented_generation_time', 'augmented_tokens_generated',\n",
    "                              'augmented_uncertainty_score', 'augmented_backtrack_events']].to_string(index=False))\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(\"\\nüìä AUGMENTED MODEL INSIGHTS\")\n",
    "            print(\"=\"*40)\n",
    "            \n",
    "            problems_with_backtracks = augmented_df[augmented_df['augmented_backtrack_events'] > 0]\n",
    "            print(f\"Problems with backtracking: {len(problems_with_backtracks)} / {len(augmented_df)}\")\n",
    "            \n",
    "            if len(problems_with_backtracks) > 0:\n",
    "                avg_backtracks = problems_with_backtracks['augmented_backtrack_events'].mean()\n",
    "                print(f\"Average backtracks per problem (when > 0): {avg_backtracks:.1f}\")\n",
    "                \n",
    "                # Show problems that benefited from backtracking\n",
    "                improved_problems = problems_with_backtracks[\n",
    "                    problems_with_backtracks['augmented_passed'] == True\n",
    "                ]\n",
    "                print(f\"Problems that passed with backtracking: {len(improved_problems)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV files: {e}\")\n",
    "        print(\"Detailed tables not available.\")\n",
    "\n",
    "# Display detailed results\n",
    "create_results_table(baseline_data, augmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusions_section"
   },
   "source": [
    "# üéâ Conclusions and Next Steps\n",
    "\n",
    "Congratulations! You've successfully run the UMinFramework benchmark and compared baseline vs augmented LLM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conclusions"
   },
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"üéØ EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"What we accomplished:\")\n",
    "print(\"‚úÖ Set up the UMinFramework environment\")\n",
    "print(\"‚úÖ Downloaded and prepared coding benchmark datasets\")\n",
    "print(\"‚úÖ Demonstrated safe code execution sandbox\")\n",
    "print(\"‚úÖ Ran baseline LLM benchmark\")\n",
    "\n",
    "if augmented_data:\n",
    "    print(\"‚úÖ Ran AugmentedLLM benchmark with uncertainty monitoring and backtracking\")\n",
    "    print(\"‚úÖ Compared performance using pass@k metrics\")\n",
    "    print(\"‚úÖ Analyzed uncertainty quantification and backtracking behavior\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AugmentedLLM benchmark not completed\")\n",
    "\n",
    "print(\"\\nüî¨ KEY INSIGHTS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "if baseline_data and augmented_data:\n",
    "    baseline_stats = baseline_data.get('statistics', {}).get('baseline', {})\n",
    "    augmented_stats = augmented_data.get('statistics', {}).get('augmented', {})\n",
    "    \n",
    "    if baseline_stats and augmented_stats:\n",
    "        baseline_pass = baseline_stats.get('pass_rate', 0)\n",
    "        augmented_pass = augmented_stats.get('pass_rate', 0)\n",
    "        \n",
    "        print(f\"‚Ä¢ Baseline model pass rate: {baseline_pass:.1%}\")\n",
    "        print(f\"‚Ä¢ Augmented model pass rate: {augmented_pass:.1%}\")\n",
    "        \n",
    "        if augmented_pass > baseline_pass:\n",
    "            print(f\"‚Ä¢ üéâ AugmentedLLM showed improvement!\")\n",
    "        elif augmented_pass < baseline_pass:\n",
    "            print(f\"‚Ä¢ üìù AugmentedLLM showed different behavior (may need tuning)\")\n",
    "        else:\n",
    "            print(f\"‚Ä¢ üìä Similar performance between models\")\n",
    "        \n",
    "        avg_uncertainty = augmented_stats.get('avg_uncertainty', 0)\n",
    "        avg_backtracks = augmented_stats.get('avg_backtrack_events', 0)\n",
    "        \n",
    "        print(f\"‚Ä¢ Average uncertainty score: {avg_uncertainty:.3f}\")\n",
    "        print(f\"‚Ä¢ Average backtrack events: {avg_backtracks:.1f}\")\n",
    "        \n",
    "        if avg_backtracks > 0:\n",
    "            print(f\"‚Ä¢ üîÑ Backtracking mechanism activated during generation\")\n",
    "        else:\n",
    "            print(f\"‚Ä¢ üìä No backtracking triggered (uncertainty below threshold)\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS\")\n",
    "print(\"=\"*20)\n",
    "print(\"1. üîß Experiment with different uncertainty thresholds\")\n",
    "print(\"2. üéØ Try larger, more capable models (e.g., meta-llama/Meta-Llama-3.1-8B-Instruct)\")\n",
    "print(\"3. üìö Train the prompt refiner for better prompt clarification\")\n",
    "print(\"4. üìä Run on larger datasets for more statistically significant results\")\n",
    "print(\"5. üî¨ Analyze specific cases where backtracking helped or hindered\")\n",
    "\n",
    "print(\"\\nüí° RESEARCH IDEAS\")\n",
    "print(\"=\"*25)\n",
    "print(\"‚Ä¢ Compare different uncertainty quantification methods\")\n",
    "print(\"‚Ä¢ Experiment with various Chain-of-Thought prompt templates\")\n",
    "print(\"‚Ä¢ Test on different domains (math, reasoning, creative writing)\")\n",
    "print(\"‚Ä¢ Investigate the relationship between model size and uncertainty calibration\")\n",
    "print(\"‚Ä¢ Study the impact of different backtracking window sizes\")\n",
    "\n",
    "print(\"\\nüìÑ REPRODUCIBILITY\")\n",
    "print(\"=\"*25)\n",
    "print(\"All results have been saved to the results/ directory:\")\n",
    "print(\"‚Ä¢ JSON files with full details and metadata\")\n",
    "print(\"‚Ä¢ CSV files for easy analysis in spreadsheet tools\")\n",
    "print(\"‚Ä¢ Pass@k metrics calculated using proper statistical methods\")\n",
    "print(\"‚Ä¢ Detailed logs for debugging and analysis\")\n",
    "\n",
    "print(\"\\nüéì Thank you for exploring the UMinFramework!\")\n",
    "print(\"   Visit the repository for more examples and documentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_section"
   },
   "source": [
    "# üíæ Export Results\n",
    "\n",
    "Download your results for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_results"
   },
   "outputs": [],
   "source": [
    "# Prepare results for download\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a zip file with all results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_filename = f\"UMinFramework_Results_{timestamp}.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # Add baseline results if available\n",
    "    baseline_dir = Path(\"results/colab_baseline\")\n",
    "    if baseline_dir.exists():\n",
    "        for file in baseline_dir.glob(\"*\"):\n",
    "            if file.is_file():\n",
    "                zipf.write(file, f\"baseline/{file.name}\")\n",
    "    \n",
    "    # Add augmented results if available\n",
    "    augmented_dir = Path(\"results/colab_augmented\")\n",
    "    if augmented_dir.exists():\n",
    "        for file in augmented_dir.glob(\"*\"):\n",
    "            if file.is_file():\n",
    "                zipf.write(file, f\"augmented/{file.name}\")\n",
    "    \n",
    "    # Add this notebook (if you want to save the executed version)\n",
    "    # Note: In Colab, you might want to download the notebook separately\n",
    "\n",
    "print(f\"üì¶ Results packaged in: {zip_filename}\")\n",
    "print(f\"üìÅ File size: {Path(zip_filename).stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# In Colab, you can download the file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_filename)\n",
    "    print(\"‚¨áÔ∏è Download started!\")\n",
    "except ImportError:\n",
    "    print(\"üí° To download in Colab: files.download('{}') \".format(zip_filename))\n",
    "    print(\"   (Uncomment the import and download lines above)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}